# Embedding Privacy: How GDPR Transformed Corporate Privacy Discourse in S&P 500 Filings
## Code Repository for Applied Social Data Science MSc Capstone (MY 498)

---

Novel text-as-data methods provide new opportunities to study the effect of legislation on corporate privacy discourse. This paper uses a word embeddings approach to study the annual 
disclosure reports of the largest US firms. Leveraging the mandatory financial disclosure regime and publicly available data on US publicly traded companies, 
this paper explores how the conceptualization of privacy changes in corporate communications before, during, and after the EU General Data Protection Regulation (GDPR). 
Embeddings are trained on a corpus of 3,719 annual reports of S&P 500 companies between 2010 and 2022.

---

# Getting Started

- Make sure to install the `requirements.txt` file to get all the appropriate libraries required to run this code.

# Explanation of Code & File Structure

- `FIGS` contains the data visualizations and some of their underlying datasets
- `RESULTS` contains the bootstrapped pairwise cosine similarity scores for each risk and advantage word

There are four Jupyter notebooks containing the code used for the entire data collection and analysis pipeline. The breakdown is as follows:

- `001_clean_parse_filings.ipynb` downloads and parses the data and conducts some exploratory analysis.
  - Downloads the dataset from Hugging Face and filters the filings to include only the companies and text columns of interest.
  - Once the data is downloaded to `csv`, an exploratory analysis is conducted, examining the usage and location of the word "privacy" in the final dataset. All visualizations generated in this notebook are stored in the `FIGS` directory.

- `002_generate_embeddings.ipynb` contains the text preprocessing and model training code:
  - preprocesses and prepares the annual filing text to train embeddings.
  - Converts the tabular data to Word2Vec ready format for training, and saves each year's corpus as a `.pkl` file in the directory called `CORPUS`.
  - Defines the Word2Vec training function and calculates pairwise cosine similarity scores across 50 bootstrap iterations.
  - Each year's model is stored in a directory called `BASE_MODELS`. Results are stored in a directory called `RESULTS`. Both `CORPUS` and `BASE_MODELS` contain data too large for GitHub, but this underlying data
can be generated by running this code.

- `003_final_analysis.ipynb` uses the bootstrapped cosine similarity results in the `RESULTS` dictionary to generate instructive visualizations.
  
-  `004_chow_test.ipynb` is not used in the final analysis or findings, but explores ways to add statistical rigor to the cosine similarity results. These findings are addressed in the paper's Limitations section.
